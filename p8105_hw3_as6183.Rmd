---
title: "p8105_hw3_as6183"
output: github_document
---

# Problem 1:
```{r}
library(p8105.datasets)
library(tidyverse)
library(plyr)
library(dplyr)
data("instacart") #loading instacart data set
```
The Instacart data set gives information on over 3 million online grocery orders from more than 200,000 Instacart users in 2017. The size of this dataset is `r nrow(instacart)` rows and `r ncol(instacart)` columns. Some key variables would be the reordered variable to assess what customers are re-buying frequently, the product name to analyze what products are customers buying most and least frequently, the user id to keep track of which customer is buying what, and the department to analyze which category of items are getting purchased most/least frequently. 

```{r}
number_aisles = count(instacart,vars=c('aisle','aisle_id'))%>%
  nrow() #named variable number_aisles because it gives the number of aisles in the data set

most_ordered = instacart%>%
  count(vars=c('aisle','aisle_id'))%>%
  top_n(3,wt=freq)  #named variable most_ordered because it gives the three aisles that have the most orders in the data set
```
There are `r number_aisles` aisles and the top 3 aisles that have the most orders are `r pull(most_ordered,aisle)`.

```{r}
library(ggplot2)
aisles_10000 = instacart%>%
  count(vars=c('aisle','aisle_id'))%>%
  filter(freq>10000) #named variable aisles_10000 because it only contains aisles in the data set with more than 10,000 orders

ggplot(aisles_10000,aes(x=aisle,y=freq))+
  geom_bar(stat="identity")+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90),plot.title=element_text(size=11))+
  xlab("Aisle Name")+
  ylab("Number of Items Ordered")+
  ggtitle("Distribution of the number of items ordered for aisles with over 10,000 orders")
```

From the plot, there are 39 aisles that have over 10,000 items ordered from them. Two of the aisles, fresh fruits and fresh vegetables, have an extremely large amount of orders, around 150,000 for both aisles. This would make sense as a majority of households eat fresh fruits and vegetables and would buy them to consume throughout the week. Contrarily, aisles such as butter or cream have a very low number of orders which could be due to the fact that a lot of healthy eaters or customers that don't consume dairy won't buy from these aisles, or in the case of paper goods contain items that only specific shoppers need. 

```{r}
#library(data.table)
data_bi_dfc_pvf = instacart%>%
  filter(aisle=="baking ingredients"|aisle=="dog food care"|aisle=="packaged vegetables fruits") #named variable this to indicate that this is a subset of the instacart data set only containg the aisles: baking ingredients (bi), dog food care (dfc), and packaged vegetables fruits (pvf).

most_popular = data_bi_dfc_pvf%>%
  count(vars=c('aisle','product_name')) #named variable most_popular to indicate that the variable gives information about the most popular aisles that customers order from

most_popular = most_popular%>%
  group_by(aisle)%>%
  dplyr::arrange(desc(freq),by_group=T)%>%
  top_n(n=3,wt=freq)

summarise(most_popular,Aisle = aisle,Product = product_name, `Times Ordered` = freq)
```
This table shows the three most popular items from the packaged vegetables fruits, baking ingredients, and dog food care aisles. Organic Baby Spinach was the most ordered item from packaged vegetables fruits, Light Brown Sugar was the most ordered item from baking ingredients, and Snack Sticks Chicken & Rice Recipe Dog Treats was the most ordered item from dog food care. This makes sense because spinach is a very popular packaged vegetable, brown sugar is needed for almost any baking recipe, and dog treats are needed to help train/feed dogs.

```{r}
pink_lady_coffee_ice_cream = instacart%>%
  filter(product_name=="Pink Lady Apples"|product_name=="Coffee Ice Cream")%>%
  group_by(order_dow,product_name) #named variable pink_lady_coffee_ice_cream to show that it gives information about the two products, Pink Lady Apples and Coffee Ice Cream. 
  
table = dplyr::summarise(pink_lady_coffee_ice_cream,mean=mean(order_hour_of_day))%>%
  ungroup()%>%
  pivot_wider(names_from=order_dow,values_from=mean) #named variable table as it puts the information from the pink_lady_coffee_ice_cream variable into a tabular format
colnames(table) = c("Product Name", "Monday", "Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
table
```

This table shows the mean hour of the day when Coffee Ice Cream and Pink Lady Apples are ordered for every day of the week. The average hour that Coffee Ice Cream was ordered is generally much later than Pink Lady Apples. This could be due to people generally wanting to eat ice cream later in the day, such as after dinner or as a midnight snack, when compared to Pink Lady Apples.

# Problem 2:
```{r}
setwd("/Users/13038/Downloads")
accel = read.csv("accel_data.csv") #reading in accel data set, named variable the same name as the data set.
accel = accel%>%
  mutate(week = factor(week))%>%
  mutate(weekday_weekend = factor(recode(day,Monday="Weekday",Tuesday="Weekday",Wednesday="Weekday",Thursday="Weekday",Friday="Weekday",Saturday="Weekend",Sunday="Weekend")))%>%
  pivot_longer(activity.1:activity.1440,names_to="Activity_Minute",values_to="Activity_Counts")
```
After I have done some data cleaning and preparation for the next steps of the problem such as pivoting the data set longer, there are `r nrow(accel)` observations in this data set corresponding to the number of days in which observations were taken and there are `r ncol(accel)` variables which corresponds to the week,day id , day, Activity minute (one row per minute of every day), Activity Counts which gives the value of the activity at that particular minute given in the column next to it, and the weekday vs weekend variable. The value for each activity time variable gives the activity counted at that particular minute of the day. Furthermore, in my tidying procedure I have converted the week variable into a factor since the week is a categorical variable. I also pivoted the activity variables to the longer format as I thought it would be easier to manipulate in the next step when I have to aggregate all of the activity's together by day. 

```{r}
total_activity_per_day=aggregate(pull(accel,Activity_Counts),by = list(pull(accel,week),pull(accel,day)),FUN=sum) #named variable total_activity_per_day as it aggregates information from the accel data set to give the total amount of activity the patient did per day instead of per minute. 
accel = accel %>%
  pivot_wider(names_from="Activity_Minute",values_from="Activity_Counts")%>%
  mutate(total_activity_per_day = pull(total_activity_per_day,x))%>%
  group_by(week)%>%
  dplyr::arrange(desc(total_activity_per_day),by_group=T)
summarise(.data=accel,Week=week,Day=day,Total_Activity_Per_Day=total_activity_per_day)
```

The table above shows the total activity per day for each day that accelerometer data was recorded,ordered by descending total activity per day so I can analyze which days of the week have higher or lower activity. A trend that is apparent to me is that the total activity seems to be higher, in general, on Thursdays and Fridays when compared to other days of the week. Furthermore, it seems that Tuesdays are a very low activity day for this person judging from the table. 

```{r}
ggplot(data=accel,aes(x=day_id,y=total_activity_per_day,color=day))+
  geom_point(size=3)+
  xlab("Day ID")+
  ylab("Total Activity Per Day")+
  ggtitle("Total Activity Measured Per Day Over 5 Weeks")
```

From this plot, I see a very similar pattern to the table above in that Thursday and Fridays seem to have more activity whereas Tuesdays seem to have the least activity, in general. There also seems to be two outliers in week 2 on Wednesday and Friday of that week which could indicate that the patient did not wear the accelerometer on those days. Another thing I noticed is that the activity totals were more spread out by day initally but in the later days of the study the total activity per day was more consistent. This could be due to the patient developing an activity routine and thus becoming more consistent throughout the week with his activity as a result. 

# Problem 3:
```{r}
library(p8105.datasets)
data("ny_noaa") #reading in ny_noaa data set
```

The ny_noaa data set contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The data set contains information from all New York state weather stations such as precipitation in tenths of a mm (prcp), snowfall in mm (snow), snow depth in mm (snwd), maximum temperature (tmax), minimum temperature (tmin), as well as the date (date) and station (id) where the information was collected. Missing data is definitely an issue in this data set as there are `r colSums(is.na(ny_noaa))` rows by column in the data set that contain missing values. Some of these columns have almost 50% of the data missing which is a very large amount. 

```{r}
ny_noaa=separate(ny_noaa,col="date",into=c("year","month","day"),sep="-")
ny_noaa=ny_noaa%>%
  dplyr::arrange(year,month)%>%
  mutate(prcp=prcp/100,snow=snow/10,snwd=snwd/10,tmax=as.integer(tmax)/10,tmin=as.integer(tmin)/10) #converted length measurements to centimeters and temperature measurements to degrees from tenths of a degree
freq_snowfall = data.frame(table(pull(ny_noaa,snow)))%>%
  top_n(n=3,wt=Freq)#named variable freq_snowfall because it gives the frequency of snowfall measures
```

For snowfall, the most commonly observed values are `r pull(freq_snowfall,Var1)` cm. This is probably because there is not that much snow between the months of April through October or November in most areas of the United States, including New York. Therefore, for most of the year you will see 0 cm or a low amount, such as 1.3 or 2.5 cm, of snowfall recorded which is why it makes sense that 0, 1.3, and 2.5 cm are the most commonly observed values for snowfall. 

```{r}
data_avg_tmax = aggregate(pull(ny_noaa,tmax),by = list(pull(ny_noaa,month),pull(ny_noaa,year),pull(ny_noaa,id)),FUN=mean,na.rm=T) #named variable data_avg_tmax because it is a data subset from the ny_noaa data set that aggregates information about the maximum temperature by the mean.
data_avg_tmax = filter(data_avg_tmax,Group.1=="01"|Group.1=="07")
data_avg_tmax = data_avg_tmax%>%
  mutate(Group.1 = recode(Group.1,"01"="January","07"="July"))
data_avg_tmax = na.omit(data_avg_tmax)
ggplot(data_avg_tmax, aes(x = Group.2, y = x)) +
  geom_boxplot()+
  facet_grid(. ~ Group.1)+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90),plot.title=element_text(size=11))+
  xlab("Year")+
  ylab("average Maximum Temperature (degrees Celcius)")+
  ggtitle("Average Max Temperatures at Each Station in January and July each Year")
```

This plot shows the average of the maximum temperatures per year in January and July at every station in the data set. The average maximum temperature for these two months seems to vary every year and does not really have a distinct pattern to it. However, there are definitely some outliers in the data set. For example, in July of 1988 there is a clear outlier as well as January 1982 and January 2005. 

```{r}
#named variable data_tmax_tmin because it seperates the data from ny_noaa by the maximum temperature and minimum temperature, making it easier to create a facet plot in the next step.
data_tmax_tmin = select(ny_noaa,c("id","year","month","day","tmax","tmin"))%>%
  pivot_longer(tmax:tmin,names_to="Temperature",values_to="Temperature_Val")%>%
  mutate(date = paste0(month,"-",day,"-",year))%>%
  na.omit()%>%
  select(-c("month","day","year"))
ggplot(data_tmax_tmin, aes(x = date, y = Temperature_Val)) +
  geom_line(size=0.25,alpha=0.2)+
  stat_smooth()+
  facet_grid(. ~Temperature)+
  theme_bw()+
  theme(axis.text.x = element_blank(),plot.title=element_text(size=10), axis.ticks.x = element_blank())+
  xlab("Time")+
  ylab("Temperature(degrees Celcius)")+
  ggtitle("Max Temperatures and Min Temperatures at Each Station for Each Day In Data Set")
```

This plot shows the maximum and minimum temperatures for the full data set. Overall from both graphs we can see the same trend of a slight increase leading to a spike in temperature and then a decrease in temperature values. Also, we can see that there are a variety of different maximum and minimum temperatures as the lines at every time interval (corresponding to the station record) varies greatly. I think this plot definitely gives a good idea of the range of snowfall values for stations over time and summarizes the overall trend of snowfall very nicely. 

```{r}
data_snow_0_10 = ny_noaa%>%
  filter(snow<10&snow>0)#named variable data_snow_0_10 because it only contains data from the ny_noaa data set where the snowfall is above 0 cm and below 10 cm (100 mm). 
data_snow_0_10 = data_snow_0_10%>%
  group_by(year)
data_snow_0_10 = data.frame(table(pull(data_snow_0_10,year)))
ggplot(data_snow_0_10,aes(x=Var1,y=Freq))+
  geom_bar(stat="identity")+
    theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90),plot.title=element_text(size=11))+
  xlab("Year")+
  ylab("Frequency of Snowfall between 0 and 10 cm (100 mm)")+
  ggtitle("Distribution of Snowfall between 0 and 10 cm or 100 mm for every year")
```
From the graph above, it can be seen that there is a significant trend upward in the amount of times snowfall between 0 and 100 mm was recorded starting in 1998. This could be due to a decrease in snowfall from 1998 onwards when compared to previous years, thus causing more values between 0 and 100 mm to be recorded by weather stations. This makes sense due to climate change and global warming that has been occurring over the past several years. 