---
title: "p8105_hw3_as6183"
output: github_document
---

# Problem 1:
```{r}
library(p8105.datasets)
library(tidyverse)
library(plyr)
library(dplyr)
data("instacart") #loading instacart data set
```
The Instacart data set gives information on over 3 million online grocery orders from more than 200,000 Instacart users in 2017. The size of this dataset is `r nrow(instacart)` rows and `r ncol(instacart)` columns. Some key variables would be the reordered variable to assess what customers are re-buying frequently, the product name to analyze what products are customers buying most and least frequently, the user id to keep track of which customer is buying what, and the department to analyze which category of items are getting purchased most/least frequently. 

```{r}
number_aisles = count(instacart,vars=c('aisle','aisle_id'))%>%
  nrow() #named variable number_aisles because it gives the number of aisles in the data set

most_ordered = instacart%>%
  count(vars=c('aisle','aisle_id'))%>%
  top_n(3,wt=freq)  #named variable most_ordered because it gives the three aisles that have the most orders in the data set
```
There are `r number_aisles` aisles and the top 3 aisles that have the most orders are `r pull(most_ordered,aisle)`.

```{r}
library(ggplot2)
aisles_10000 = instacart%>%
  count(vars=c('aisle','aisle_id'))%>%
  filter(freq>10000) #named variable aisles_10000 because it only contains aisles in the data set with more than 10,000 orders

ggplot(aisles_10000,aes(x=aisle,y=freq))+
  geom_bar(stat="identity")+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90),plot.title=element_text(size=11))+
  xlab("Aisle Name")+
  ylab("Number of Items Ordered")+
  ggtitle("Distribution of the number of items ordered for aisles with over 10,000 orders")
```

From the plot, there are 39 aisles that have over 10,000 items ordered from them. Two of the aisles, fresh fruits and fresh vegetables, have an extremely large amount of orders, around 150,000 for both aisles. This would make sense as a majority of households eat fresh fruits and vegetables and would buy them to consume throughout the week. Contrarily, aisles such as butter or cream have a very low number of orders which could be due to the fact that a lot of healthy eaters or customers that don't consume dairy won't buy from these aisles, or in the case of paper goods contain items that only specific shoppers need. 

```{r}
#library(data.table)
data = instacart%>%
  filter(aisle=="baking ingredients"|aisle=="dog food care"|aisle=="packaged vegetables fruits")

most_popular = data%>%
  count(vars=c('aisle','product_name'))

most_popular = most_popular%>%
  group_by(aisle)%>%
  dplyr::arrange(desc(freq),by_group=T)%>%
  top_n(n=3,wt=freq)

summarise(most_popular,Aisle = aisle,Product = product_name, `Times Ordered` = freq)
```
This table shows the three most popular items from the packaged vegetables fruits, baking ingredients, and dog food care aisles. Organic Baby Spinach was the most ordered item from packaged vegetables fruits, Light Brown Sugar was the most ordered item from baking ingredients, and Snack Sticks Chicken & Rice Recipe Dog Treats was the most ordered item from dog food care. This makes sense because spinach is a very popular packaged vegetable, brown sugar is needed for almost any baking recipe, and dog treats are needed to help train/feed dogs.

```{r}
pink_lady_coffee_ice_cream = instacart%>%
  filter(product_name=="Pink Lady Apples"|product_name=="Coffee Ice Cream")%>%
  group_by(order_dow,product_name)
  
table = dplyr::summarise(pink_lady_coffee_ice_cream,mean=mean(order_hour_of_day))%>%
  ungroup()%>%
  pivot_wider(names_from=order_dow,values_from=mean)
colnames(table) = c("Product Name", "Monday", "Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
table
```

This table shows the mean hour of the day when Coffee Ice Cream and Pink Lady Apples are ordered for every day of the week. The average hour that Coffee Ice Cream was ordered is generally much later than Pink Lady Apples. This could be due to people generally wanting to eat ice cream later in the day, such as after dinner or as a midnight snack, when compared to Pink Lady Apples.

# Problem 2:
```{r}
setwd("/Users/13038/Downloads")
accel = read.csv("accel_data.csv")
accel = accel%>%
  mutate(week = factor(week))%>%
  mutate(weekday_weekend = factor(recode(day,Monday="Weekday",Tuesday="Weekday",Wednesday="Weekday",Thursday="Weekday",Friday="Weekday",Saturday="Weekend",Sunday="Weekend")))%>%
  pivot_longer(activity.1:activity.1440,names_to="Activity_Minute",values_to="Activity_Counts")
```
There are `r nrow(accel)` observations in this data set corresponding to the number of days in which observations were taken and there are `r ncol(accel)` variables which corresponds to the week,day id , day, Activity minute (one row per minute of every day), Activity Counts which gives the value of the activity at that particular minute given in the column next to it, and the weekday vs weekend variable. The value for each activity time variable gives the activity counted at that particular minute of the day. Furthermore, in my tidying procedure I have converted the week variable into a factor since the week is a categorical variable. I also pivoted the activity variables to the longer format as I thought it would be easier to manipulate in the next step when I have to aggregate all of the activity's together by day. 

```{r}
total_activity_per_day=aggregate(pull(accel,Activity_Counts),by = list(pull(accel,week),pull(accel,day)),FUN=sum)
accel = accel %>%
  pivot_wider(names_from="Activity_Minute",values_from="Activity_Counts")%>%
  mutate(total_activity_per_day = pull(total_activity_per_day,x))
summarise(.data=accel,Week=week,Day=day,Total_Activity_Per_Day=total_activity_per_day)
```

```{r}
ggplot(data=accel,aes(x=day_id,y=total_activity_per_day,color=day))+
  geom_point(size=3)+
  xlab("Day ID")+
  ylab("Total Activity Per Day")+
  ggtitle("Total Activity Measured Per Day Over 5 Weeks")
```

# Problem 3:
```{r}
library(p8105.datasets)
data("ny_noaa")
```

The ny_noaa data set contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The data set contains information from all New York state weather stations such as precipitation in tenths of a mm (prcp), snowfall in mm (snow), snow depth in mm (snwd), maximum temperature (tmax), minimum temperature (tmin), as well as the date (date) and station (id) where the information was collected. Missing data is definitely an issue in this data set as there are `r colSums(is.na(ny_noaa))` rows by column in the data set that contain missing values. Some of these columns have almost 50% of the data missing which is a very large amount. 

```{r}
ny_noaa=separate(ny_noaa,col="date",into=c("year","month","day"),sep="-")
ny_noaa=ny_noaa%>%
  dplyr::arrange(year,month)%>%
  mutate(prcp=prcp/100,snow=snow/10,snwd=snwd/10,tmax=as.integer(tmax)/10,tmin=as.integer(tmin)/10) #converted length measurements to centimeters and temperature measurements to degrees from tenths of a degree
mode_snowfall = data.frame(table(pull(ny_noaa,snow)))
```

For snowfall, the most commonly observed values are `r top_n(mode_snowfall,n=1,wt=Freq)[1]` cm. This is probably because there is not that much snow between the months of April through October or November in most areas of the United States, including New York. Therefore, for most of the year you will see 0 cm of snowfall recorded which is why it is the most commonly observed value for snowfall. 

```{r}
data = aggregate(pull(ny_noaa,tmax),by = list(pull(ny_noaa,month),pull(ny_noaa,year),pull(ny_noaa,id)),FUN=mean,na.rm=T)
data = filter(data,Group.1=="01"|Group.1=="07")
data = data%>%
  mutate(Group.1 = recode(Group.1,"01"="January","07"="July"))
data = na.omit(data)
ggplot(data, aes(x = Group.2, y = x)) +
  geom_boxplot()+
  facet_grid(. ~ Group.1)+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90),plot.title=element_text(size=11))+
  xlab("Year")+
  ylab("average Maximum Temperature (degrees Celcius)")+
  ggtitle("Average Max Temperatures at Each Station in January and July each Year")
```

```{r}
data = select(ny_noaa,c("id","year","month","day","tmax","tmin"))%>%
  pivot_longer(tmax:tmin,names_to="Temperature",values_to="Temperature_Val")%>%
  mutate(date = paste0(month,"-",day,"-",year))%>%
  na.omit()%>%
  select(-c("month","day","year"))
ggplot(data, aes(x = date, y = Temperature_Val)) +
  geom_line()+
  facet_grid(. ~Temperature)+
  theme_bw()+
  theme(axis.text.x = element_blank(),plot.title=element_text(size=10), axis.ticks.x = element_blank())+
  xlab("Time")+
  ylab("Temperature(degrees Celcius)")+
  ggtitle("Max Temperatures and Min Temperatures at Each Station for Each Day In Data Set")

data = filter(ny_noaa,snow<10&snow>0)
data = aggregate(pull(data,snow),by = list(pull(data,year)),FUN=sum,na.rm=T)
ggplot(data,aes(x=Group.1,y=x))+
  geom_bar(stat="identity")+
    theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90),plot.title=element_text(size=11))+
  xlab("Year")+
  ylab("Snowfall (cm)")+
  ggtitle("Distribution of Snowfall between 0 and 100 mm for every year")
```