---
title: "p8105_hw3_as6183"
output: github_document
---

# Problem 1:
```{r}
library(p8105.datasets)
library(tidyverse)
data("instacart") #loading instacart data set
```
The Instacart data set gives information on over 3 million online grocery orders from more than 200,000 Instacart users in 2017. The size of this dataset is `r nrow(instacart)` rows and `r ncol(instacart)` columns. Some key variables would be the reordered variable to assess what customers are re-buying frequently, the product name to analyze what products are customers buying most and least frequently, the user id to keep track of which customer is buying what, and the department to analyze which category of items are getting purchased most/least frequently. 

```{r}
number_aisles = instacart%>%
  dplyr::count(aisle)%>%
  nrow() #named variable number_aisles because it gives the number of aisles in the data set

most_ordered = instacart%>%
  dplyr::count(aisle)%>%
  top_n(3,wt=n)  #named variable most_ordered because it gives the three aisles that have the most orders in the data set
```
There are `r number_aisles` aisles and the top 3 aisles that have the most orders are `r pull(most_ordered,aisle)`.

```{r}
instacart%>%
  dplyr::count(aisle)%>%
  filter(n>10000)%>%
  mutate(aisle = factor(aisle))%>%
  mutate(aisle = fct_reorder(aisle,n))%>%
ggplot(aes(x=aisle,y=n))+
  geom_bar(stat="identity")+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90,vjust = 0.5, hjust = 1),plot.title=element_text(size=11))+
  xlab("Aisle Name")+
  ylab("Number of Items Ordered")+
  ggtitle("Distribution of the number of items ordered for aisles with over 10,000 orders")
```

From the plot, there are 39 aisles that have over 10,000 items ordered from them. Two of the aisles, fresh fruits and fresh vegetables, have an extremely large amount of orders, around 150,000 for both aisles. This would make sense as a majority of households eat fresh fruits and vegetables and would buy them to consume throughout the week. Contrarily, aisles such as butter or cream have a very low number of orders which could be due to the fact that a lot of healthy eaters or customers that don't consume dairy won't buy from these aisles, or in the case of paper goods contain items that only specific shoppers need. 

```{r}
#library(data.table)
data_bi_dfc_pvf = instacart%>%
  filter(
    aisle=="baking ingredients"|
    aisle=="dog food care"|
    aisle=="packaged vegetables fruits"
         ) #named variable this to indicate that this is a subset of the instacart data set only containing the aisles: baking ingredients (bi), dog food care (dfc), and packaged vegetables fruits (pvf).

most_popular = data_bi_dfc_pvf%>%
  dplyr::count(aisle,product_name)%>%
  group_by(aisle)%>%
  dplyr::arrange(desc(n),by_group=T)%>%
  top_n(n=3,wt=n) #named variable most_popular to indicate that the variable gives information about the most popular aisles that customers order from

most_popular%>%
  dplyr::summarise(Product = product_name, `Times Ordered` = n)%>%
  knitr::kable()

```
This table shows the three most popular items from the packaged vegetables fruits, baking ingredients, and dog food care aisles. Organic Baby Spinach was the most ordered item from packaged vegetables fruits, Light Brown Sugar was the most ordered item from baking ingredients, and Snack Sticks Chicken & Rice Recipe Dog Treats was the most ordered item from dog food care. This makes sense because spinach is a very popular packaged vegetable, brown sugar is needed for almost any baking recipe, and dog treats are needed to help train/feed dogs.

```{r}
pink_lady_coffee_ice_cream = instacart%>%
  filter(product_name=="Pink Lady Apples"|product_name=="Coffee Ice Cream")%>%
  group_by(order_dow,product_name) #named variable pink_lady_coffee_ice_cream to show that it gives information about the two products, Pink Lady Apples and Coffee Ice Cream. 
  
table = dplyr::summarise(pink_lady_coffee_ice_cream,mean=mean(order_hour_of_day))%>%
  ungroup()%>%
  pivot_wider(names_from=order_dow,values_from=mean) #named variable table as it puts the information from the pink_lady_coffee_ice_cream variable into a tabular format
colnames(table) = c("Product Name", "Monday", "Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
table%>%
  knitr::kable()
```

This table shows the mean hour of the day when Coffee Ice Cream and Pink Lady Apples are ordered for every day of the week. The average hour that Coffee Ice Cream was ordered is generally much later than Pink Lady Apples. This could be due to people generally wanting to eat ice cream later in the day, such as after dinner or as a midnight snack, when compared to Pink Lady Apples.

# Problem 2:
```{r}
accel = read.csv("./p8105_hw3_as6183_files/data/accel_data.csv") #reading in accel data set, named variable the same name as the data set.
accel = accel%>%
  mutate(week = factor(week))%>%
  mutate(weekday_weekend = factor(recode(day,Monday="Weekday",Tuesday="Weekday",Wednesday="Weekday",Thursday="Weekday",Friday="Weekday",Saturday="Weekend",Sunday="Weekend")))%>%
  pivot_longer(activity.1:activity.1440,names_to="Activity_Minute",values_to="Activity_Counts")
```
After I have done some data cleaning and preparation for the next steps of the problem such as pivoting the data set longer, there are `r nrow(accel)` observations in this data set corresponding to the number of days in which observations were taken and there are `r ncol(accel)` variables which corresponds to the week,day id , day, Activity minute (one row per minute of every day), Activity Counts which gives the value of the activity at that particular minute given in the column next to it, and the weekday vs weekend variable. The value for each activity time variable gives the activity counted at that particular minute of the day. Furthermore, in my tidying procedure I have converted the week variable into a factor since the week is a categorical variable. I also pivoted the activity variables to the longer format as I thought it would be easier to manipulate in the next step when I have to aggregate all of the activity's together by day. 

```{r}
accel %>%
  group_by(week,day) %>%
  dplyr::summarize(total_activity_per_day = sum(Activity_Counts))%>%
  pivot_wider(names_from = day,values_from = total_activity_per_day) %>%
  knitr::kable()
```

The table above shows the total activity per day for each day that accelerometer data was recorded. A trend that is apparent to me is that the total activity seems to be higher, in general, on Fridays when compared to other days of the week. Furthermore, it seems that Tuesdays and Wednesdays are very low activity days for this person judging from the table. On the last two Saturdays recorded, there is also a very low amount of total activity. This could be because the patient forgot to wear the device on those days.

```{r}
accel%>%
  mutate(Activity_Minute = str_remove_all(Activity_Minute,"activity."))%>%
  mutate(Activity_Minute = as.integer(Activity_Minute))%>%
  ggplot(aes(x=Activity_Minute,y=Activity_Counts,color=day))+
  geom_line()+
  xlab("Time of Day (Minutes) with 0 indicating 12:00 AM")+
  ylab("Activity Time Courses")+
  ggtitle("Activity Measured Per Day Over 5 Weeks")
```

This plot shows the activity time courses for every day recorded for every minute of each day. From the plot, there is a clear trend that the activity is very low to start with and increases throughout the day which makes sense since most people are generally asleep at the very beginning of the day and would thus not be doing any activity at that time. Furthermore, there is a clear spike in activity around evening time, which could be the time that the patient gets off work and thus he could have more time to be active at that particular time in the day. Finally, we can see that Wednesday and Saturday are the two days with the lowest activity which corresponds to what we saw in the table above. The low amount of activity on Saturday in this graph could, again, be due to the two outliers we saw in the total activity table above. 

# Problem 3:
```{r}
library(p8105.datasets)
data("ny_noaa") #reading in ny_noaa data set
missing_data = data.frame(
  Columns = colnames(ny_noaa),
  Rows_Missing_Data = colSums(is.na(ny_noaa))
  )%>%
  dplyr::summarise(Columns = Columns, `Number of Rows Missing Data` = Rows_Missing_Data)%>%
  knitr::kable() #variable gives tabular information on number of rows missing in ny_noaa data set per column
```

The ny_noaa data set contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The data set contains information from all New York state weather stations such as precipitation in tenths of a mm (prcp), snowfall in mm (snow), snow depth in mm (snwd), maximum temperature (tmax), minimum temperature (tmin), as well as the date (date) and station (id) where the information was collected. Missing data is definitely an issue in this data set as shown in the missing data by column table below. 
`r missing_data`
As shown in the table, some of these columns have almost 50% of the data missing which is a very large amount. 

```{r}
ny_noaa=separate(ny_noaa,col="date",into=c("year","month","day"),sep="-")%>%
  dplyr::arrange(year,month)%>%
  mutate(
    prcp=prcp/10,
    snow=snow,
    snwd=snwd,
    tmax=as.numeric(tmax)/10,
    tmin=as.numeric(tmin)/10
         ) #converted length measurements to millimeters and temperature measurements to degrees Celsius
freq_snowfall = ny_noaa%>%
  dplyr::count(snow)%>%
  top_n(n=3,wt=n)%>%
  dplyr::arrange(desc(n))%>%
  dplyr::summarise(`Snowfall (mm)` = snow, `Number of Observations` = n)%>%
  knitr::kable() #named variable freq_snowfall because it gives the frequency of snowfall measures
```

For snowfall, the most commonly observed values are shown in this table: `r freq_snowfall`

This is probably because there is not that much snow between the months of April through October or November in most areas of the United States, including New York. Therefore, for most of the year you will see 0 mm of snowfall recorded which is why it makes sense that 0 mm is the most commonly observed value for snowfall. The NA is the second most common missing value due to the large amount of missing data in the dataset. 

```{r}
data_avg_tmax = ny_noaa%>%
  group_by(year,month,id)%>%
  dplyr::summarise(avg_tmax = mean(tmax,na.rm=T))%>%
  filter(month=="01"|month=="07")%>%
  mutate(month = recode(month,"01"="January","07"="July")) #named variable data_avg_tmax because it is a data subset from the ny_noaa data set that aggregates information about the maximum temperature by the mean.

ggplot(data_avg_tmax, aes(x = year, y = avg_tmax, color = id)) +
  geom_point()+
  facet_grid(. ~ month)+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90,vjust = 0.5, hjust = 1),plot.title=element_text(size=11), legend.position = 'none')+
  xlab("Year")+
  ylab("average Maximum Temperature (degrees Celcius)")+
  ggtitle("Scatterplot of Average Max Temperatures at Each Station in January and July each Year")

ggplot(data_avg_tmax, aes(x = year, y = avg_tmax)) +
  geom_boxplot()+
  facet_grid(. ~ month)+
  theme_bw()+
  theme(axis.text.x = element_text(size=8,angle=90,vjust = 0.5, hjust = 1),plot.title=element_text(size=11), legend.position = 'none')+
  xlab("Year")+
  ylab("Average Maximum Temperature (degrees Celsius)")+
  ggtitle("Boxplot of Average Max Temperatures at Each Station in January and July each Year")
```

These plots shows the average of the maximum temperatures per year in January and July at every station in the data set. The first plot is a scatterplot of the data with the color of the dots corresponding to each distinct station. The second plot is a boxplot, which I plotted to see the median of the average max temperatures over time for comparison purposes. The average maximum temperature for these two months seems to vary every year and does not really have a distinct pattern to it. As expected, however, the median of the average maximum temperatures for the stations is lower in January when compared to July consistently. Finally, there are definitely some outliers that can be seen in the data set from these graphs. For example, in July of 1989,1991, and 2006 there are clear outliers that are much colder than normal as well as January 2005 which has an outlier that is much warmer than normal. 

```{r}
library(patchwork)
library(hexbin)
# plot of tmax vs tmin
tmax_tmin_plot = ggplot(ny_noaa, aes(x = tmin, y = tmax)) +
  geom_hex()+
  theme_bw()+
  theme(plot.title=element_text(size=10))+
  xlab("Min Temperatures (degrees C)")+
  ylab("Max Temperatures (degrees C)")+
  ggtitle("Max Temperatures vs Min Temperatures for the Entire Data Set")

#plot of snowfall distribution over time for values between 0 and 100 mm
snow_0_100_plot = ny_noaa%>%
  filter(snow<100&snow>0)%>%
  ggplot(aes(x=year,y=snow))+
  geom_boxplot()+
  stat_summary(fun=mean,color="red",geom="point")+
  theme(axis.text.x = element_text(size=8,angle=90,vjust = 0.5, hjust = 1),plot.title=element_text(size=10))+
  xlab("Year")+
  ylab("Snowfall (mm)")+
  ggtitle("Distribution of Snowfall between 0 and 100 mm by year")

tmax_tmin_plot/snow_0_100_plot
```

This plot shows the maximum versus minimum temperatures for the full data set on top and the distribution of snowfall values between 0 and 100 mm by year on the bottom. From the first plot of maximum vs minimum temperatures we can see that there is a significant concentration of data points that have similar maximum and minimum temperatures, showcased by the lighter blue in the center of the graph. From the second plot we can see that the median amount of snowfall between 0 and 100 mm is pretty consistent over the years. I also plotted the mean on the boxplots, showcased by the red dot in each box for each year, which can help show which years had extreme snowfall values as the mean is more prone to change when outliers are present when compared to the median. For example, 1993 and 1994 had a mean that was much larger than the median which could indicate that there are some outliers in those years that are much higher than the rest of the values. 